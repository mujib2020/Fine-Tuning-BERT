# -*- coding: utf-8 -*-
"""Fine_Tune BERT -1 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Hgrw4r39kxP_kjjp7AlkXtsaTPPe9tk

# Fine tune BERT using Keras
## Binary text classification

### Load dataset
First, load a dataset. We’ll use the CoLA dataset from the GLUE benchmark, since it’s a simple binary text classification task, and just take the training split for now.
"""

pip install datasets

pip install transformers

from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset_train = dataset["train"]  # Just take the training split for now

dataset_train.shape

dataset_train[0]

dataset_train[-1]

"""Next, load a tokenizer and tokenize the data as NumPy arrays. Note that the labels are already a list of 0 and 1s, so we can just convert that directly to a NumPy array without tokenization!"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_data = tokenizer(dataset_train['sentence'], return_tensors = "np", padding = True )

tokenized_data[-1]

# # Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras
tokenized_data = dict(tokenized_data)
tokenized_data['input_ids']

import numpy as np

# Label is already an array of 0 and 1
labels = np.array(dataset_train["label"])
labels

"""### Now we load, compile, and fit the model.
 Note that Transformers models all have a default task-relevant loss function, so we don’t need to specify one unless we want to:
"""

import tensorflow

from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile the model
model_1 = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Lower learning rates are often better for fine-tuning transformers. (# No loss argument!)
model_1.compile(optimizer=Adam(5e-5))

"""# Train the model"""

model_1.fit(tokenized_data, labels)

"""#### Evaluate the model on test dataset"""

dataset_test = dataset["test"]

dataset_test.shape

dataset_test[8]

# dataset_test["label"]

"""## No labels are given with test set"""

tokenized_test_data = tokenizer(dataset_test['sentence'], return_tensors = "np", padding = True )
tokenized_test_data[1]

tokenized_test_data = dict(tokenized_test_data)
tokenized_test_data['input_ids']

# Make predictions
predictions = model_1.predict(tokenized_test_data)

predictions

""" The predictions are logits, let's convert them to probabilities using softmax

"""

probabilities = tensorflow.nn.softmax(predictions.logits, axis=-1)

probabilities

"""The index of the highest probability can be considered as the class label (0 or 1)

"""

predicted_labels = tensorflow.argmax(probabilities, axis=1)

predicted_labels

# Output the predicted labels
predicted_labels.numpy()

"""### There is no true labels for the test set:
So, it is useful to pick out a few samples from the dataset and look at the predicted labels and the input sentences side by side.
"""

# Select a few random samples from the test set
samples = np.random.choice(len(dataset_test['sentence']), 5, replace=False)

# Display the sentences and their predicted labels
for i in samples:
    print(f"Sentence: {dataset_test['sentence'][i]}")
    print(f"Predicted Label: {predicted_labels.numpy()[i]}\n")

"""### Histogram of Predicted Classes
This can show you how many instances of each class the model has predicted.
"""

import matplotlib.pyplot as plt

# Convert predicted labels to a numpy array
predicted_labels_array = predicted_labels.numpy()

# Plot a histogram
plt.figure(figsize=(10, 6))
plt.hist(predicted_labels_array, bins=len(set(predicted_labels_array)), alpha=0.7, color='blue', edgecolor='black')

plt.title('Histogram of Predicted Classes')
plt.xlabel('Class')
plt.ylabel('Frequency')
plt.xticks(ticks=range(len(set(predicted_labels_array))), labels=[f'Class {i}' for i in range(len(set(predicted_labels_array)))])
plt.show()

"""#### Bar Chart of Prediction Counts
Similar to a histogram, a bar chart can provide a clear visual representation of the counts of each predicted class.
"""

from collections import Counter

# Count the occurrences of each predicted class
counter = Counter(predicted_labels_array)

# Get the classes and their counts
classes = list(counter.keys())
counts = list(counter.values())

# Plot a bar chart
plt.figure(figsize=(10, 6))
plt.bar(classes, counts, color='green')

plt.title('Bar Chart of Predicted Classes')
plt.xlabel('Class')
plt.ylabel('Count')
plt.xticks(ticks=classes, labels=[f'Class {i}' for i in classes])
plt.show()



